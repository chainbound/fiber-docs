"use strict";(self.webpackChunkfiber_website=self.webpackChunkfiber_website||[]).push([[1219],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>u});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),h=p(n),u=o,b=h["".concat(s,".").concat(u)]||h[u]||d[u]||r;return n?a.createElement(b,i(i({ref:t},c),{},{components:n})):a.createElement(b,i({ref:t},c))}));function u(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},7346:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=n(7462),o=(n(7294),n(3905));const r={sidebar_position:5,title:"FiberDB"},i=void 0,l={unversionedId:"fiberdb",id:"fiberdb",title:"FiberDB",description:"Introduction",source:"@site/docs/fiberdb.md",sourceDirName:".",slug:"/fiberdb",permalink:"/docs/fiberdb",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fiberdb.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,title:"FiberDB"},sidebar:"tutorialSidebar",previous:{title:"Regions",permalink:"/docs/regions"},next:{title:"FAQ",permalink:"/docs/faq"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Data Format",id:"data-format",level:2},{value:"Database",id:"database",level:2},{value:"Example",id:"example",level:2}],c={toc:p};function d(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"FiberDB is a component that collects all message observations (transactions, blocks) from all of the deployed Fiber nodes. It then separates these observations into objects and stores them in designated tables in a Clickhouse database. Format below:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Fiber-IndexDB-ERD.png",src:n(2519).Z,width:"2625",height:"2380"})),(0,o.kt)("p",null,"The most interesting tables are the observations table. Since we have global points of presence in the form of Fiber Nodes, that connect to both the eth1 (execution) and eth2 (consensus) networks, we can link traffic on both p2p networks to geographic regions."),(0,o.kt)("p",null,"For the eth1 network, this allows us to identify the big transaction originators. This will be some of the biggest RPC node providers. Customers will be able to use this data to strategically place their clients. Since we also store application-level data (the full transaction), it\u2019s possible to link app-specific data to the network layer. This would be useful in identifying PGAs for example."),(0,o.kt)("p",null,"For the eth2 network, we can locate validators, staking pools and relays. We can also analyze equivocations and other anomalies on the consensus network."),(0,o.kt)("h2",{id:"data-format"},"Data Format"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"strong"},"transaction_observations")))),(0,o.kt)("p",null,"All of the observations of a certain transaction (identified by ",(0,o.kt)("inlineCode",{parentName:"p"},"tx_hash"),") through the Fiber network. More details below."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"strong"},"private_transaction_observations")))),(0,o.kt)("p",null,"Same as above but for private (user) transactions. Only accessible by the user that sent the transaction for tracing / debugging purposes."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"strong"},"transactions")))),(0,o.kt)("p",null,"All of the individual transaction along with their payloads. This is what you would see on Etherscan. The reason that we index these as well is that not all transactions sent over the network eventually end up on the chain, and some of them get lost / replaced. We want to know what types of these transactions are as well. One example use case of this would be identifying PGAs - a lot of the replaced transactions won\u2019t end up on the chain, but since we index their payload, we can easily look into what was going on."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"strong"},"block_observations")))),(0,o.kt)("p",null,"All of the observations of a certain block (identified by ",(0,o.kt)("inlineCode",{parentName:"p"},"block_hash")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"block_number"),") through the Fiber network. This allows us to see, at a network level, where blocks are originated and how they propagate. It will also allow us to trace potential equivocation attacks or slashable offenses more closely."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"strong"},"block_headers")))),(0,o.kt)("p",null,"Header payloads. Same as with the transactions table, allows us to examine blocks that didn\u2019t make it on the canonical chain more closely."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},(0,o.kt)("inlineCode",{parentName:"strong"},"block_bodies")))),(0,o.kt)("p",null,"Contain all of the transaction hashes in that block. Cross-checking these with the ",(0,o.kt)("inlineCode",{parentName:"p"},"transaction_observations")," table will allow us to identify \u201cprivate\u201d transactions (i.e. not broadcasted over the p2p network)."),(0,o.kt)("p",null,"Let\u2019s take a closer look at the ",(0,o.kt)("inlineCode",{parentName:"p"},"transaction_observations")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"block_observations")," tables. Each row in these tables is an observation from one of our nodes, along with some network metadata like ",(0,o.kt)("inlineCode",{parentName:"p"},"timestamp"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"region"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"node_id")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"source"),". The ",(0,o.kt)("inlineCode",{parentName:"p"},"source")," is the peer we received the message from."),(0,o.kt)("p",null,"The observations are further categorized by ",(0,o.kt)("inlineCode",{parentName:"p"},"observation_type"),". This column describes where the message came from, and it can either be from a peer in the p2p network (",(0,o.kt)("inlineCode",{parentName:"p"},"p2p"),"), or from another Fiber node (",(0,o.kt)("inlineCode",{parentName:"p"},"fiber"),"). All transactions are propagated over the internal Fiber network for lower latency, which is why we have this column to distinguish the 2."),(0,o.kt)("p",null,"For any message observation (unless it\u2019s the first), there will likely be a row with the ",(0,o.kt)("inlineCode",{parentName:"p"},"fiber")," type first, and a ",(0,o.kt)("inlineCode",{parentName:"p"},"p2p")," type second, because the Fiber network is usually faster than the p2p network. We record both types so that we can analyze the delta between them, as well as the normal p2p propagation behaviour."),(0,o.kt)("h2",{id:"database"},"Database"),(0,o.kt)("p",null,"This data is backed by a hosted ",(0,o.kt)("a",{parentName:"p",href:"https://clickhouse.com/"},"Clickhouse")," database. We chose Clickhouse Cloud due to its impressive performance, automatic up/down scaling, and great UX. As a customer, there are 2 ways for you to get this data:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Realtime Push"),": our indexing middleware would connect directly to your database, and all messages will be pushed to the DB as soon as the indexer sees them. You would be able to define what data you want to save. This option is the most flexible, because you\u2019ll be able to do whatever you want with the data once you have it locally."),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Hosted DB Access"),": you get access to our hosted Clickhouse DB. We create a user with limited quota\u2019s, with which you can connect and do everything you need. Less flexible, but no need to store anything locally.")),(0,o.kt)("h2",{id:"example"},"Example"),(0,o.kt)("p",null,"Let\u2019s look at an example transaction trace:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"FiberDB Example Trace",src:n(4179).Z,width:"1297",height:"706"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The transaction was first received by node ",(0,o.kt)("inlineCode",{parentName:"li"},"fiber-node-nzB22n0B-eu-central-1")," in Frankfurt."),(0,o.kt)("li",{parentName:"ul"},"It then sent it on to the Fiber network to all of the other nodes, and the nodes in Europe got the transaction soon after. With a small delay, they also received it from the p2p network (check the ",(0,o.kt)("inlineCode",{parentName:"li"},"observation_type")," column)"),(0,o.kt)("li",{parentName:"ul"},"However, in the US, it was received by node ",(0,o.kt)("inlineCode",{parentName:"li"},"fiber-node-Bhg6kcGw-us-east-1")," from the ",(0,o.kt)("inlineCode",{parentName:"li"},"p2p")," network first. It then propagated it on to the other nodes in the US, and we see the same thing happening as in the previous step."),(0,o.kt)("li",{parentName:"ul"},"Then, in AP, node ",(0,o.kt)("inlineCode",{parentName:"li"},"fiber-node-jnv6bfld-ap-east-1")," in Hong Kong picked up the transaction from a peer and broadcasts it on.")))}d.isMDXComponent=!0},2519:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/fiberdb-erd-0a3bb54a0ce8d73b9ed96d44f6770e92.png"},4179:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/fiberdb-example-a53c8d7d095f6ccf966914d426f11805.png"}}]);