"use strict";(self.webpackChunkfiber_website=self.webpackChunkfiber_website||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"fiber-vs-bloxroute","metadata":{"permalink":"/blog/fiber-vs-bloxroute","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-23-fiber-vs-bloxroute/index.md","source":"@site/blog/2023-02-23-fiber-vs-bloxroute/index.md","title":"Fiber vs. Bloxroute: The Standoff","description":"Source//www.pinterest.com/pin/567242515550180347","date":"2023-02-23T00:00:00.000Z","formattedDate":"February 23, 2023","tags":[{"label":"ethereum","permalink":"/blog/tags/ethereum"},{"label":"mempool","permalink":"/blog/tags/mempool"}],"readingTime":6.705,"hasTruncateMarker":true,"authors":[{"name":"Jonas Bostoen","title":"Chainbound CTO","url":"https://github.com/jonasbostoen","imageURL":"https://github.com/jonasbostoen.png","key":"jonas"}],"frontMatter":{"slug":"fiber-vs-bloxroute","title":"Fiber vs. Bloxroute: The Standoff","authors":["jonas"],"tags":["ethereum","mempool"]},"nextItem":{"title":"Diving into the Reth p2p stack","permalink":"/blog/reth-p2p"}},"content":"<figure>\\n<img src=\\"https://i.pinimg.com/564x/ca/98/5e/ca985ebc3cfc605d9e0d314b807b8353.jpg\\"/>\\n<figcaption><sup>Source: https://www.pinterest.com/pin/567242515550180347</sup></figcaption>\\n</figure>\\n\\n## Introduction\\n\\nFor the last few months, we\u2019ve been working on a new implementation of Fiber on Ethereum. Fiber is a mempool service that consists of a network of highly optimized nodes. \\n\\nFiber is built with 3 pillars in mind:\\n\\n- Reliability\\n- (consistent) Performance\\n- Resource-efficiency\\n\x3c!-- truncate --\x3e\\n\\nThese pillars, and the fact that there\u2019s now a new, modular execution client being built by Paradigm ([https://github.com/paradigmxyz/reth](https://github.com/paradigmxyz/reth)) in Rust, made Rust the ideal candidate for the job. And so we set to work. A couple of months later now, and we\u2019ve deployed the new version. \\n\\nBloxroute has been the de-facto mempool service for traders and searchers on Ethereum. Back in December 2020, KeeperDAO (now Rook) ran a [performance benchmark](https://medium.com/keeperdao/a-performance-benchmark-on-mempool-services-9e68bf070952) on Bloxroute, Blocknative and a standard Alchemy node. They found that Bloxroute blew the other 2 out of the water: it delivered transactions first 97% of the time. Since then, there hasn\u2019t really been any new competition, - we aim to change that.\\n\\nOur [benchmarking tool](https://github.com/chainbound/fiber-benchmarks) consistently shows Fiber seeing a transaction first about 80% of the time, with a median latency difference of 10-12 milliseconds. \\n\\nThis is the output of the last benchmarking run (1 hour)[^1]:\\n```\\n========== STATS =============\\nMean difference: 34.81ms\\nMedian difference: 11.00ms\\nP10 difference: -2.00ms\\nP25 difference: 3.00ms\\nP75 difference: 29.00ms\\nP90 difference: 53.00ms\\nMax difference: 9311.00ms\\nMin difference: -81.00ms\\nStdev: 273.80ms\\n\\n========== RESULT =============\\nFiber won 81.87% of the time\\n```\\nThis is great news for Fiber, but seeing a pending transaction first is only one side of the coin. \\nIn real-world scenarios, HFT players and bots need to capitalize on these opportunities by broadcasting \\na responding transaction as quickly as possible[^2]. \\nWe therefore decided to measure both listening and broadcasting, to see which service is faster on both fronts.\\n\\n## Setup\\n:::info\\n**Price plans used:**\\n\\n- **Bloxroute**: Enterprise Plan ($1250/mo)\\n- **Fiber**: Basic Plan ($500/mo)\\n:::\\n\\nTo test the services, we\u2019ll set up a classical MEV scenario: the backrun. \\n\\n*Backrunning is the act of sending a transaction with the same gas price as some target \\ntransaction, as fast as possible, which will result in your transaction being ordered right\\nafter the target transaction by the entity putting together the block[^3]. Transactions with \\nthe same gas price are ordered by arrival time, so latency is extremely important here[^4]. \\nLiquidity / NFT sniping, arbitrage and liquidations are all types of backrunning.*\\n\\nBackrunning will allow us to test 2 interrelated components that a mempool service must \\nhave separately: **fast transaction delivery** (listening side) and **fast transaction \\ndistribution** (sending side / broadcasting). The combination of these 2 is what determines \\noverall performance. We will establish the backrunning bot on a server in the us-east-1 Virginia \\nregion, with roughly 1 millisecond latency to both the Fiber and Bloxroute API \\n(we\u2019re using the Cloud Websockets API). \\nWe don\u2019t want last hop latency messing with the results.\\n\\nOur target transactions (the one we want to backrun), will be broadcast from a couple of different sources: Infura, Alchemy, and a residential node in Belgium. The reason for this is that Infura and Alchemy together originate most of Ethereum transactions, and for the residential node, we\u2019ll see how the services compare for a transaction that comes from somewhere closer to the network edge as opposed to AWS or Google Cloud. It will also show us the internal network latency of both network services: if a transaction originates in Belgium and our bot is in Virginia, a large part of the total latency will be taken up by internal propagation. Since both solutions use AWS, we don\u2019t expect a large difference here, but it could still have an impact. Besides that, we\u2019re also going to be backrunning 30 random transactions, that could come from anywhere.\\n\\nWe\u2019re testing 10 backruns on each RPC node (besides the 30 random ones), to see if we can spot a pattern. The backrun transaction will be sent with the same nonce to both Bloxroute and Fiber, so to differentiate, we\u2019ll input the service name as UTF-8 in the transaction\u2019s calldata. For same nonce transactions, only the first one will be included, while the other will be dropped.\\n\\n* Fiber: `0x6669626572`\\n![](./fiber-input.png)\\n* Bloxroute: `0x626c6f78726f757465`\\n![](./bloxroute-input.png)\\n\\n## Results\\n:::info\\nDownload the CSV dataset [here](./data.csv).\\n:::\\n### Transaction delivery\\n\\nFor delivery time, Fiber was first 52 out of 60 times, \\nwhich is **87%** and roughly corresponds to our benchmark run above.\\n![](./seen-first-rate.png)\\nLet\u2019s take a closer look at transaction delivery time per service. \\nThe vertical axis is the transaction delivery latency in **milliseconds**. \\nThe service that first discovered the transaction has a latency of 0, and \\nthe other value is the difference between the two.\\n![](./all-differences.png)\\nThe largest latency difference is **166ms** in favor of Fiber, \\nlet\u2019s filter it out for now so we can have a better look at the rest of the values:\\n![](./filtered-differences.png)\\nThe above graph shows us that often, it\u2019s a close call, but overall, **Fiber is much closer to the baseline of 0 in all of the cases**. \\nThe largest difference by which Fiber lost is **5 milliseconds**, compared to 166ms for Bloxroute. \\nThis shows us that Fiber is faster, and has more consistent performance, \\nwhich satisfies our second requirement.\\n\\nWe don\u2019t find any significant correlation between the differences in delivery time and the RPC node used to \\nbroadcast the target transaction, which means that both services are pretty well connected globally, and are able to internally propagate transactions quickly.\\n\\n### Overall performance\\n**Fiber wins in 54 out of 60 backruns (90% of the time)**\\n![](./total-win-rate.png)\\nFiber wins 90% of the backrunning challenges that we set up.\\n\\nInterestingly, in 2 cases where Bloxroute delivered a transaction first (by 1 and 4 milliseconds), Fiber still won the backrun. This means that in those 2 cases, Fiber\u2019s distribution network was faster.\\n<figure>\\n<img src=\\"https://wallpapercave.com/wp/wp9032145.jpg\\"/>\\n<figcaption><sup>Source: https://wallpapercave.com/w/wp9032145</sup></figcaption>\\n</figure>\\n\\n## Conclusion\\n**A good mempool service tries to tame the chaos and randomness inherent to a global mempool**. This gets harder and harder the more decentralized a network layer is, which is why it\u2019s important to try to cover as much ground as possible. Not just in terms of geographic distribution, but also in connecting to a broad set of peers. With Fiber, we\u2019ve built the technology in a way that allows us to do this for a reasonable cost, which is why we came out ahead. \\n\\nEver since the [Flashbots auction](https://docs.flashbots.net/flashbots-auction/overview) launched, we realize latency on Ethereum mainnet has lost some of its importance. But we decided to build on Ethereum first to prove that we can be faster, and to then expand to other chains (and rollups) to do the same. Check out our [roadmap](https://fiber.chainbound.io/docs/roadmap) for more information.\\n\\nIf you\u2019re interested in trying Fiber out, we have a 5 day refund period. Join our [Discord](https://discord.gg/J4KNdeCYGX) and shoot us a message and we\u2019ll DM you. For now, we offer 2 plans: Basic ($500/mo) and Professional ($1000/mo). The only differentiator between the plans is that with Basic, you can actively listen to only one stream at a time, while you can listen to up to 3 with Professional (from multiple locations). We don\u2019t introduce any artificial latency to the Basic plan. Accessing Fiber can easily be done by using the client libraries we built in [Go](https://github.com/chainbound/fiber-go), [Rust](https://github.com/chainbound/fiber-rs) and [TypeScript](https://github.com/chainbound/fiber-ts). Read more in the documentation [here](https://fiber.chainbound.io/docs/intro).\\n\\n**Thanks for reading!**\\n\\n## Footnotes\\n[^1]: P10/P25/P75/P90 are the percentiles in which each of the latency difference observations fall. Negative means that Bloxroute was faster. For example, P10 is -2ms, which means that for the lowest 10% of observations, Bloxroute was faster by >2ms.\\nOn the other hand, a P90 value of 53 means that 10% of the transaction where delivered by Fiber first with a difference of more than 54 milliseconds.\\n[^2]: Unless they decide to use a private mempool like Flashbots.\\n[^3]: Assuming neither the backrun transaction or the target transaction is reordered due to being included in a bundle.\\n[^4]: Go-ethereum code: [go-ethereum/core/types/transaction.go#L53](https://github.com/ethereum/go-ethereum/blob/645e3e86c45bfb726b507e661f788baa85d0249f/core/types/transaction.go#L531). Note that for type 2 transactions, the order is determined by the priority fee."},{"id":"reth-p2p","metadata":{"permalink":"/blog/reth-p2p","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-02-17-reth-p2p/index.md","source":"@site/blog/2023-02-17-reth-p2p/index.md","title":"Diving into the Reth p2p stack","description":"Recently, the engineering team at Paradigm unveiled a new open-source execution client","date":"2023-02-17T00:00:00.000Z","formattedDate":"February 17, 2023","tags":[{"label":"ethereum","permalink":"/blog/tags/ethereum"},{"label":"devp2p","permalink":"/blog/tags/devp-2-p"},{"label":"reth","permalink":"/blog/tags/reth"},{"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":11.39,"hasTruncateMarker":true,"authors":[{"name":"Jonas Bostoen","title":"Chainbound CTO","url":"https://github.com/jonasbostoen","imageURL":"https://github.com/jonasbostoen.png","key":"jonas"}],"frontMatter":{"slug":"reth-p2p","title":"Diving into the Reth p2p stack","authors":["jonas"],"tags":["ethereum","devp2p","reth","rust"]},"prevItem":{"title":"Fiber vs. Bloxroute: The Standoff","permalink":"/blog/fiber-vs-bloxroute"},"nextItem":{"title":"Identifying hotspots on the Ethereum p2p network","permalink":"/blog/ethereum-hotspots"}},"content":"Recently, the engineering team at Paradigm [unveiled](https://www.paradigm.xyz/2022/12/reth) a new open-source execution client \\ncalled **[Reth](https://github.com/paradigmxyz/reth)**. In short, reth aims to be an execution client to serve a large \\nuser base, including **power users** like MEV searchers, RPC node operators, block builders, and more. It will do so by making reth highly \\nperformant, but also very configurable, so that users can tweak and tune the settings to find the optimal setup \\nfor their use case. No other execution client is doing this, so this is big positive for the space.\\n\\n\x3c!-- truncate --\x3e\\n\\n![](./client-diversity.png)\\n\\nAside from this and the fact that [client diversity](https://clientdiversity.org/#distribution) is crucial, \\nreth aims to accomodate developers as well. \\n**Every part of the reth stack will be modular enough to use as a standalone component in your stack**. \\nIn their article, Paradigm mentions the blazing-fast database bindings that indexing companies could use, \\nand that their EVM can be leveraged by [ERC4337](https://eips.ethereum.org/EIPS/eip-4337) bundlers to run fast simulations. \\nIn this article, we\u2019ll talk about the architecture and usage of the networking stack, which plays an important role in [Fiber Network](https://fiber.chainbound.io).\\n\\n:::info\\n\ud83d\udca1 When I\u2019m referencing reth code, you will find a Github permalink in the footnotes. \\n\\nAll the example code (including dependencies and versions) can be found at [https://github.com/jonasbostoen/reth-p2p-showcase](https://github.com/jonasbostoen/reth-p2p-showcase).\\n:::info\\n\\n## Showcase\\n\\nWe don\u2019t just want to tell you about how great it is, we want to show you. Below is a Rust code snippet of how you can spin up a minimal [devp2p](https://github.com/ethereum/devp2p) node and listen to network events:\\n\\n```rust title=\\"examples/simple.rs\\"\\nuse secp256k1::{rand, SecretKey};\\nuse std::sync::Arc;\\nuse tokio_stream::StreamExt;\\n\\nuse reth_network::{NetworkConfig, NetworkManager};\\nuse reth_provider::test_utils::NoopProvider;\\n\\n#[tokio::main]\\nasync fn main() -> anyhow::Result<()> {\\n    // Generate a random ECDSA private key.\\n    let secret = SecretKey::new(&mut rand::thread_rng());\\n\\n    // Create the network builder with the secret key, which allows you to configure the network.\\n    // The defaults will do for now, but we\'ll disable DNS discovery\\n\\t\\t// and specify the bootnodes discv4 should use for discovering peers.\\n\\t\\tlet builder = NetworkConfig::<NoopProvider>::builder(secret)\\n        .disable_dns_discovery()\\n        .boot_nodes(mainnet_nodes());\\n\\n    // Build the config. The configuration needs a client to interact with the chain (to respond to block header\\n    // and block bodies requests), but for now we\'ll just use a no-op client.\\n    let config = builder.build(Arc::new(NoopProvider::default()));\\n    let network = NetworkManager::new(config).await?;\\n\\n    // Get a handle to the network manager\\n    let network_handle = network.handle().clone();\\n    // Subscribe to network events\\n    let mut network_events = network_handle.event_listener();\\n    println!(\\"Starting network manager...\\");\\n    // Spawn the network manager task. This will start the network manager\\n    // and all the subcomponents necessary for the devp2p stack.\\n    tokio::spawn(network);\\n\\n    while let Some(net_event) = network_events.next().await {\\n        println!(\\"Received network event: {:?}\\", net_event);\\n    }\\n\\n    Ok(())\\n}\\n```\\nAll of the complexities of discovery, peer and session management, and message handling are nicely bundled inside of the `NetworkManager`. We\'ll cover\\nthat in more details below, but for now, you can just run it with\\n```\\ncargo r --example simple\\n```\\n\\nAfter a while you should start to see network events like `PeerAdded`, `PeerRemoved`, `SessionEstablished` and `SessionClosed`[^1]:\\n\\n![](./screenshot-1.png)\\n\\nNote that this is a very minimal example and you won\u2019t be able to maintain any peering connections, because for that, we would need to respond to `eth` protocol \\nrequests like [`GetBlockHeaders`](https://github.com/ethereum/devp2p/blob/master/caps/eth.md#getblockheaders-0x03) and [`GetBlockBodies`](https://github.com/ethereum/devp2p/blob/master/caps/eth.md#getblockbodies-0x05), \\nwhich we can only do if we provide a valid chain provider (instead of `NoopProvider`).\\n\\nThis doesn\u2019t give us a lot of information though. Reth uses the `tracing` library for logging events, \\nwhich we can activate with the `RUST_LOG` environment variable.\\n\\nIf we run our binary with `RUST_LOG` set to `trace`, `debug` or `info`, we can see a lot more of what\u2019s happening under the hood:\\n```\\nRUST_LOG=debug cargo r --example simple\\n```\\n\\n![](./screenshot-2.png)\\n\\nIf you want to zoom in on certain trace targets you can do so by tuning the `RUST_LOG` variable, for example, \\nsetting it to `RUST_LOG=discv4=trace,net=info`.\\n\\n### Creating our own message handlers\\n\\nWe were able to run a minimally functional devp2p node in about 20 lines of code, which is impressive. But let\u2019s take a look at what else the modular design allows us to do. \\nIn this following example, we\u2019ll set the basis for providing custom handlers for `eth` protocol messages like `GetBlockHeaders` and `GetBlockBodies`, \\nbut also for the transaction exchange messages:\\n\\n- `PooledTransactionHashes`\\n- `Transactions`\\n- `PooledTransactions`\\n\\nThis could be useful if you want to route `eth` sync requests to another location (say you\u2019re running this as a sentry and want to relay requests to another node that keeps the actual blockchain state). \\nOr you want to build your own transaction pool logic so you need access to all the transaction messages you\u2019re receiving from the network. We can do this by using the `set_eth_request_handler` and `set_transactions` methods on our `NetworkManager` like so:\\n\\n```rust title=\\"examples/custom_handlers.rs\\"\\nuse reth_p2p::init_tracing;\\nuse secp256k1::{rand, SecretKey};\\nuse std::sync::Arc;\\nuse tokio::sync::mpsc;\\nuse tokio_stream::StreamExt;\\n\\nuse reth_discv4::bootnodes::mainnet_nodes;\\nuse reth_network::{NetworkConfig, NetworkManager, PeersConfig};\\nuse reth_provider::test_utils::NoopProvider;\\n\\n#[tokio::main]\\nasync fn main() -> anyhow::Result<()> {\\n    init_tracing();\\n    // Generate a random ECDSA private key.\\n    let secret = SecretKey::new(&mut rand::thread_rng());\\n\\n    // // Create a peer configuration with max 10 outbound and 10 inbound peers.\\n    let peer_config = PeersConfig::default()\\n        .with_max_outbound(10)\\n        .with_max_inbound(10);\\n\\n    // Add the peer configuration here.\\n    let builder = NetworkConfig::<NoopProvider>::builder(secret)\\n        .disable_dns_discovery()\\n        .boot_nodes(mainnet_nodes())\\n        .peer_config(peer_config);\\n\\n    // Build the config. The configuration needs a client to interact with the chain (to respond to block header\\n    // and block bodies requests), but for now we\'ll just use a no-op client.\\n    let config = builder.build(Arc::new(NoopProvider::default()));\\n    let mut network = NetworkManager::new(config).await?;\\n\\n    // Create the channels for receiving eth messages\\n    let (eth_tx, mut eth_rx) = mpsc::unbounded_channel();\\n    let (transaction_tx, mut transaction_rx) = mpsc::unbounded_channel();\\n\\n    network.set_eth_request_handler(eth_tx);\\n    network.set_transactions(transaction_tx);\\n\\n    let network_handle = network.handle().clone();\\n    let mut network_events = network_handle.event_listener();\\n    println!(\\"Starting network manager...\\");\\n    tokio::spawn(network);\\n\\n    loop {\\n        tokio::select! {\\n            Some(tx_event) = transaction_rx.recv() => {\\n                println!(\\"New transaction event: {:?}\\", tx_event);\\n            }\\n\\n            Some(eth_req) = eth_rx.recv() => {\\n                println!(\\"New eth protocol request: {:?}\\", eth_req);\\n            }\\n\\n            Some(net_event) = network_events.next() => {\\n                println!(\\"New network event: {:?}\\", net_event);\\n            }\\n        }\\n    }\\n}\\n```\\n\\nWhen you run the `custom_handlers` example now, you will occasionally see these messages printed out. Note that in this example, \\nI\u2019ve also used a custom peer configuration that specifies the maximum number of outbound and inbound peers. \\nNow that we have a very slim idea of what we can do with `reth-network`, let\u2019s have a closer look at the architecture before looking at some more options.\\n\\n## Architecture\\nAt a high level, the p2p stack is implemented as a hierarchy of [state machines](https://en.wikipedia.org/wiki/Finite-state_machine) where parents pass events down to their children, and children bubble up events to their parents. \\n`NetworkManager`[^2] is the entry point of this whole hierarchy, and it is implemented as an endless [future](https://rust-lang.github.io/async-book/02_execution/02_future.html). \\nWhen we spawn the manager with `tokio::spawn(network)`, we let the tokio runtime drive the state of the manager forward. \\nThe runtime continually calls `poll`[^3], which in turn polls all the underlying components, all the way down the hierarchy. \\nThis mechanism is how the events bubble up and work is performed[^4]. The architecture in a diagram looks like this:\\n\\n![](./reth-network-architecture.png)\\n\\n`Discv5` is greyed out because it\u2019s still in the works. One thing we\u2019ve seen in the code but haven\u2019t talked about yet is the `NetworkHandle`, which we got with `network.handle().clone()`. This is a thread-safe, clonable handle that can be used to communicate with the network manager and all of the other components it manages. \\nIf you look at the whole reth codebase, you\u2019ll see that this handle is shared between all the different subsystems that need access to the network, like the state syncer. \\nThis pattern is somewhat similar to the [actor pattern](https://ryhl.io/blog/actors-with-tokio/).\\n\\nFor us, it also has a very interesting interface. We can use it to broadcast transactions or transaction announcements, add and remove (trusted) peers, change peer reputation, and so on. \\nTake a look at [this file](https://github.com/paradigmxyz/reth/blob/24bc633bbda05de0772c66b851af83c63a028144/crates/net/network/src/network.rs) for more information. \\nWe won\u2019t cover what each component does in too much depth, since that\u2019s already being done in the [reth developer docs](https://github.com/paradigmxyz/reth/tree/main/docs).\\n\\n### Peer lifecycle\\n\\nLet\u2019s take a look at the lifecycle of a peer in the networking stack, because this will involve every component we\u2019ve outlined above. \\nSpecifically, we\u2019ll look at outbound connections, because it\'s a more complex process.\\n\\n`Discovery`[^5] is the service responsible for discovering peers. It currently envelops 2 subsystems: `Discv4`[^6] and `DnsDiscovery`[^7], with `Discv5` in the [pipeline](https://github.com/paradigmxyz/reth/issues/1383). \\nIf you want to dive deeper into how these mechanisms work, see the links in the footnotes. Discovering a peer is the first step in the peer lifecycle. \\nIn short, the discovery service queries other nodes over UDP for their neighbors, starting with the pre-defined bootstrap nodes. The nodes in a response message\\nare queried in turn, and so on, so that our node gradually builds up a picture of the network. Every node undergoes a couple of checks, and if passed, is considered a newly discovered node.\\n\\nOnce a new node is discovered, the discovery service will bubble up an event with the information of the peer to the `NetworkState`[^8], which in turn notifies the `Swarm`[^9]. \\nThe `Swarm` in then does an [EIP-2124](https://eips.ethereum.org/EIPS/eip-2124) fork ID check. If this check passes, the `Swarm` adds the peer to the `PeerManager`, where it will sit as a candidate to be dialed[^10].\\n\\nDepending on wether there are any outbound peer slots available, the peer will be bubbled up as a dial candidate to the `Swarm` again. \\nThis time, it will pass it down to `SessionManager` to start an outbound session with[^11]. \\nSpecifically, the `Swarm` calls [`dial_outbound`](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/session/mod.rs#L215), \\nwhich will try to establish a session with the peer by running the [RLPx](https://github.com/ethereum/devp2p/blob/master/rlpx.md) and [eth](https://github.com/ethereum/devp2p/blob/master/caps/eth.md) protocol handshakes. \\nOnce these handshakes pass, the peer will move from the pending to the active state, \\nand a new `ActiveSession` will be created, which wraps the underlying RLPx and `eth` connection with said peer[^12]. \\nThe `SessionManager` keeps track of all these sessions, and notifies its parents about any messages, like transactions or block header requests.\\n\\nWe\u2019ve skipped over some parts of the work involved, but hopefully you now have some idea about how these components interact, and how there is a clean [seperation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns).\\n\\n## More examples\\n\\nWhat if you want to bypass the default discovery services and only add peers that you\u2019ve evaluated somehow? This evaluation function could be anything, like\\n\\n- Ping the remote endpoint and only go ahead if RTT is below a certain threshold (contrived example, don\'t do this in prod)\\n- Check if the IP address belongs to a certain cloud provider or is in a certain country / region\\n- \u2026\\n\\nFor this example we\u2019ll use the `Discv4` service, but you could also use the `Discovery` service, which does both discv4 and DNS discovery. \\nFirst we\u2019ll disable the default discovery services, and then spawn our own discv4 service. \\nWe listen to the discovery events, and when a new node is added, we run the evaluation function, only proceeding with adding the peer\\nif it evaluates to `true`:\\n\\n```rust title=\\"examples/custom_disc.rs\\"\\nuse reth_p2p::init_tracing;\\nuse secp256k1::{rand, SecretKey};\\nuse std::{net::SocketAddr, str::FromStr, sync::Arc, time::Duration};\\nuse tokio_stream::StreamExt;\\n\\nuse reth_discv4::{\\n    bootnodes::mainnet_nodes, DiscoveryUpdate, Discv4, Discv4ConfigBuilder, NodeRecord,\\n};\\nuse reth_network::{NetworkConfig, NetworkManager};\\nuse reth_network_api::{PeerKind, Peers};\\nuse reth_provider::test_utils::NoopProvider;\\n\\n#[tokio::main]\\nasync fn main() -> anyhow::Result<()> {\\n    init_tracing();\\n    // Generate a random ECDSA private key.\\n    let secret = SecretKey::new(&mut rand::thread_rng());\\n\\n    // Disable the default discovery services.\\n    let builder = NetworkConfig::<NoopProvider>::builder(secret)\\n        .disable_dns_discovery()\\n        .disable_discv4_discovery()\\n        .boot_nodes(mainnet_nodes());\\n\\n    // Build the config. The configuration needs a client to interact with the chain (to respond to block header\\n    // and block bodies requests), but for now we\'ll just use a no-op client.\\n    let config = builder.build(Arc::new(NoopProvider::default()));\\n    let network = NetworkManager::new(config).await?;\\n\\n    let peer_id = *network.peer_id();\\n\\n    // Get a handle to the network manager\\n    let network_handle = network.handle().clone();\\n    println!(\\"Starting network manager...\\");\\n    // Spawn the network manager task. This will start the network manager\\n    // and all the subcomponents necessary for the devp2p stack.\\n    tokio::spawn(network);\\n\\n    // Build local node record\\n    let disc_addr = SocketAddr::from_str(\\"0.0.0.0:30303\\").unwrap();\\n    let local_enr = NodeRecord {\\n        id: peer_id,\\n        address: disc_addr.ip(),\\n        tcp_port: disc_addr.port(),\\n        udp_port: disc_addr.port(),\\n    };\\n\\n    // Create the discv4 config\\n    let discv4_config = Discv4ConfigBuilder::default()\\n        // Decrease lookup interval to 5 seconds (from 20 sec default)\\n        .lookup_interval(Duration::from_secs(5))\\n        // Decrease ban duration to 30 minutes\\n        .ban_duration(Some(Duration::from_secs(30 * 60)))\\n        .add_boot_nodes(mainnet_nodes())\\n        .build();\\n\\n    let (_discv4, mut service) = Discv4::bind(disc_addr, local_enr, secret, discv4_config).await?;\\n    let mut disc_updates = service.update_stream();\\n\\n    // Spawn the discv4 service\\n    let _handle = service.spawn();\\n\\n    while let Some(disc_event) = disc_updates.next().await {\\n        match disc_event {\\n            DiscoveryUpdate::Added(enr) | DiscoveryUpdate::DiscoveredAtCapacity(enr) => {\\n                println!(\\"Discovered new node: {:?}\\", enr);\\n                // Evaluate if we want to connect to peer\\n                if custom_peer_eval_func(&enr) {\\n                    network_handle.add_peer(enr.id, enr.tcp_addr());\\n                }\\n            }\\n            DiscoveryUpdate::Removed(id) => {\\n                network_handle.remove_peer(id, PeerKind::Basic);\\n            }\\n            _ => {}\\n        }\\n    }\\n\\n    Ok(())\\n}\\n\\n// This function could be used to add some custom peer evaluation logic,\\n// like a ping below n milliseconds.\\nfn custom_peer_eval_func(_enr: &NodeRecord) -> bool {\\n    true\\n}\\n```\\n\\nWe\u2019ve also tweaked some settings here like the default ban duration and the lookup interval, which will have an effect on how fast you\u2019ll discover new nodes. Note that you\u2019ll want to run the peer evaluation function asynchronously if you\u2019re doing any network I/O like pings, to not block the event handling loop.\\n\\n### Other ideas\\n\\n- Use the `DiscoveryService` to build a network crawler.\\n- You can change the reputation of the peers you\u2019re connected to through the `NetworkHandle::reputation_change`[^13]. \\nThere are some default reputation penalties that reth implements on bad messages, disconnections, etc., \\nbut you can apply your own with `ReputationChangeKind::Other`. This could allow you to build a peer set that meets certain requirements.\\n\\nThanks for reading and feel free to contact me on Twitter [@mempirate](https://twitter.com/mempirate) if you have any questions!\\n\\n## Further reading\\n\\n- [Reth developer docs](https://github.com/paradigmxyz/reth/tree/main/docs)\\n- The code documentation (run `cargo doc --open` in the repository)\\n\\n## Footnotes\\n\\n[^1]: [https://github.com/paradigmxyz/reth/blob/c5bc272057b473337191de50e7f86d07cf93c5a1/crates/net/network/src/manager.rs#L771](https://github.com/paradigmxyz/reth/blob/c5bc272057b473337191de50e7f86d07cf93c5a1/crates/net/network/src/manager.rs#L771)\\n[^2]: [https://github.com/paradigmxyz/reth/blob/c5bc272057b473337191de50e7f86d07cf93c5a1/crates/net/network/src/manager.rs#L86](https://github.com/paradigmxyz/reth/blob/c5bc272057b473337191de50e7f86d07cf93c5a1/crates/net/network/src/manager.rs#L86)\\n\\n[^3]: [https://github.com/paradigmxyz/reth/blob/](https://github.com/paradigmxyz/reth/blob/24bc633bbda05de0772c66b851af83c63a028144/crates/net/network/src/manager.rs#L466)[c5bc272057b473337191de50e7f86d07cf93c5a1](https://github.com/paradigmxyz/reth/blob/c5bc272057b473337191de50e7f86d07cf93c5a1/crates/net/network/src/manager.rs#L86)[/crates/net/network/src/manager.rs#L466](https://github.com/paradigmxyz/reth/blob/24bc633bbda05de0772c66b851af83c63a028144/crates/net/network/src/manager.rs#L466)\\n\\n[^4]: This is similar to how rust-libp2p does it: [https://github.com/libp2p/rust-libp2p/blob/master/docs/coding-guidelines.md](https://github.com/libp2p/rust-libp2p/blob/master/docs/coding-guidelines.md)\\n\\n[^5]: [https://github.com/paradigmxyz/reth/blob/6005ecb89a42b34facdaa020471347b47b043526/crates/net/network/src/discovery.rs#L23](https://github.com/paradigmxyz/reth/blob/6005ecb89a42b34facdaa020471347b47b043526/crates/net/network/src/discovery.rs#L23)\\n[^6]: [https://github.com/paradigmxyz/reth/blob/6005ecb89a42b34facdaa020471347b47b043526/crates/net/discv4/src/lib.rs#L119](https://github.com/paradigmxyz/reth/blob/6005ecb89a42b34facdaa020471347b47b043526/crates/net/discv4/src/lib.rs#L119)\\n        \\n      Spec: [https://github.com/ethereum/devp2p/blob/master/discv4.md](https://github.com/ethereum/devp2p/blob/master/discv4.md)\\n[^7]: [https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/dns/src/lib.rs#L86](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/dns/src/lib.rs#L86)\\n      \\n      Spec: [https://eips.ethereum.org/EIPS/eip-1459](https://eips.ethereum.org/EIPS/eip-1459)\\n\\n[^8]: [https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/state.rs#L45](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/state.rs#L45)\\n[^9]: [https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/swarm.rs#L65](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/swarm.rs#L65)\\n[^10]: [https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/peers/manager.rs#L73](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/peers/manager.rs#L73)\\n[^11]: [https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/session/mod.rs#L52](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/session/mod.rs#L52)\\n[^12]: [https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/session/mod.rs#L370](https://github.com/paradigmxyz/reth/blob/f9de425ad895279c24a72977ffd0c6973afaf90e/crates/net/network/src/session/mod.rs#L370)\\n[^13]: [https://github.dev/paradigmxyz/reth/blob/7c9b212b4a60838ce35c3db272788bd05d2eeec0/crates/net/network/src/network.rs#L215](https://github.dev/paradigmxyz/reth/blob/7c9b212b4a60838ce35c3db272788bd05d2eeec0/crates/net/network/src/network.rs#L215)"},{"id":"ethereum-hotspots","metadata":{"permalink":"/blog/ethereum-hotspots","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-10-21-ethereum-hotspots/index.md","source":"@site/blog/2022-10-21-ethereum-hotspots/index.md","title":"Identifying hotspots on the Ethereum p2p network","description":"In our journey of creating the ideal network topology for our Fiber nodes, we had to determine the critical geographical","date":"2022-10-21T00:00:00.000Z","formattedDate":"October 21, 2022","tags":[{"label":"ethereum","permalink":"/blog/tags/ethereum"},{"label":"devp2p","permalink":"/blog/tags/devp-2-p"}],"readingTime":7.425,"hasTruncateMarker":true,"authors":[{"name":"Jonas Bostoen","title":"Chainbound CTO","url":"https://github.com/jonasbostoen","imageURL":"https://github.com/jonasbostoen.png","key":"jonas"}],"frontMatter":{"slug":"ethereum-hotspots","title":"Identifying hotspots on the Ethereum p2p network","authors":["jonas"],"tags":["ethereum","devp2p"]},"prevItem":{"title":"Diving into the Reth p2p stack","permalink":"/blog/reth-p2p"}},"content":"In our journey of creating the ideal network topology for our Fiber nodes, we had to determine the critical geographical\\nregions of the Ethereum p2p network. We will define these \\"hotspots\\" as regions where the most transactions are originally\\nbroadcasted from.\\n\\nBecause Fiber is a mempool service, we need to be able to deliver transactions to users as fast as possible, which also means making sure we sufficiently cover these hotspots. In essence, we want to make sure we\'re as close to the original\\nbroadcaster is possible, which in p2p terms means either being directly connected to it, or only 1 or 2 hops away. Each hop\\nwill introduce a non-trivial amount of latency, which is something we need to minimize.\\n\\n\x3c!-- truncate --\x3e\\n\\nLet\'s look at a couple scenarios:\\n\\n![](./2-hop-setup.png)\\n\\nIn this setup, we\'re 2 hops away from the transaction broadcaster. We\'ll simplify here and assume that nodes always\\nbroadcast the full transaction message, which is not the case [[2]](#references).\\n\\nThere are 3 factors introducing latency here:\\n* **A**: The first network hop.\\n* **B**: The time it takes this node to process, validate, and re-broadcast the transaction.\\n* **C**: The second network hop.\\n\\nThis network topology is not ideal, since your total latency will be **A + B + C**, and all of these steps\\nhave an element of variability to them.\\n\\nWhat we actually want is the following, single hop topology:\\n\\n![](./1-hop-setup.png)\\n\\nHere, the only latency we have is **A**, the first and only network hop. The only way to achieve this is to try\\nto **connect to as much peers as possible in that region**, which will give you a better chance of direct peerings\\nwith transaction broadcasters. You can do this by either having one node with a very large maximum peer count, or by having more\\nthan one node in the region. Because maintaining a large amount of peers is pretty resource intensive, we opted\\nfor the second option.\\n\\nNow that we know why we need to cover these hotspots sufficiently, how do we identify them?\\nThis article will go over our findings.\\n\\n## The setup\\nThe setup will consist of a number of Ethereum nodes deployed globally that **keep track of when they received a certain transaction, but also from which peer it came**. \\nLater on, we\'ll be able to inspect these timestamps to see which node heard about the transaction first. \\nWe\'ll also be able to use the peer IP addresses to find which hosting providers are responsible for broadcasting\\nthe most transactions. But where do we deploy these nodes?\\n\\nIt\'s no secret that **most critical Ethereum infrastructure runs on AWS** [[1]](#references):\\n\\n![](./hosting-chart.png)\\n\\nWhich is why we deployed our nodes in the following 13 AWS regions:\\n* us-east-1 (N. Virginia)\\n* us-east-2 (Ohio)\\n* us-west-1 (N. California)\\n* us-west-2 (Oregon)\\n* ap-east-1 (Hong Kong)\\n* ap-northeast-2 (Seoul)\\n* ap-northeast-1 (Tokyo)\\n* ap-southeast-1 (Singapore)\\n* eu-west-1 (Ireland)\\n* eu-west-2 (London)\\n* eu-west-3 (Paris)\\n* eu-north-1 (Stockholm) \\n\\nThis obviously doesn\'t cover the whole world, but since they\'re located at backbone hotspots, it will do.\\n\\nEach node will be connected to a significant amount of peers (400), **for a theoretical\\nmaximum reach of 5200 peers**. In practice, this number will be a lot lower due to duplicate peers.\\nWith this setup, we\'ve recorded **5.5 million Ethereum transactions** over the period of a couple of days. Let\'s look at the results.\\n\\n## The results\\n\\n### Geography\\n![](./tx_geo.png)\\nMost transactions are seen first in the US in the North Virginia area, which is\\nwhere the notorious **us-east-1** AWS region sits. Ohio is a hotspot as well, seeing\\na little more than 8% of transactions first. Regions on the West Coast are less active:\\nOregon and North California combined only see a little more than 5% of transactions first.\\n\\nIn **Europe**, Frankfurt is by far the most active region. Paris and London are also important\\nregions, both seeing close to 6% of recorded transactions first. In the **Asia Pacific** region, Tokyo and Singapore are the most active.\\n\\n\\nInterestingly, looking at the activity per \\"continent\\", we can see that the EU and the US lead by a wide margin:\\n![](./tx_geo_continent.png)\\n\\nBased on these results, we\'ve been able to work out a good geographical distribution of our node setup.\\n\\n### Hosting providers\\nSince we recorded the sending peer of every transaction, we can take a closer look at the different hosting\\nproviders that originate the most transactions. We know that AWS hosts a lot of full nodes, but is that actually where\\ninfrastructure that belongs to the largest *transaction broadcasters* is?\\n\\nTo figure that out, we\'ll look at the top 3 most \\"active\\" peers (peers from which we received the most transactions first) per region, and identify the hosting provider based on their IP addresses.\\n\\n**us-east-1: Virginia region**\\n\\nAll in AWS, Virginia.\\n\\n**eu-central-1: Frankfurt region**\\n\\nAll in Hetzner, Frankfurt. Hetzner is a bare metal cloud provider that\'s primarily active in Germany.\\n\\n**us-east-2: Ohio region**\\n\\nAll in AWS, Virginia. Only the 4th biggest originated from the AWS Ohio datacenter, again confirming that the us-east-1 region\\nis a big hotspot.\\n\\n**eu-west-3: Paris region**\\n1. Google Cloud, Brussels\\n2. Google Cloud, Brussels\\n3. Hetzner, Frankfurt\\n\\nThis seems to suggest that the Paris region itself is not very active.\\n\\n**eu-west-2: London region**\\n\\n1. OVHCloud, London\\n2. Google Cloud, Brussels\\n3. Hetzner, Frankfurt\\n\\n**ap-northeast-1: Tokyo region**\\n\\n1. AWS, Tokyo\\n2. AWS, Virginia\\n3. AWS, Tokyo\\n\\n**ap-southeast-1: Singapore region**\\n\\n1. Contabo, Singapore\\n2. Hetzner, Frankfurt (?)\\n3. AWS, Singapore\\n\\n**eu-west-1: Dublin region**\\n\\n1. AWS, Dublin\\n2. AWS, Tokyo (?)\\n3. AWS, Dublin\\n\\n**us-west-2: Oregon region**\\n\\n1. Telus Communications, Vancouver\\n2. AWS, Virginia\\n3. AWS, Oregon\\n\\n**ap-east-1: Hong Kong region**\\n\\n1. Google Cloud, Hong Kong\\n2. Google Cloud, Hong Kong\\n3. AWS, Virginia (?)\\n\\n**ap-northeast-2: Seoul region**\\n\\n1. AWS, Seoul\\n2. AWS, Seoul\\n3. AWS, Tokyo\\n\\n**eu-north-1: Stockholm region**\\n\\n1. Hetzner, Frankfurt\\n2. AWS, Virginia (?)\\n3. Hetzner, Finland\\n\\n**us-west-1: California region**\\n\\n1. AWS, Virginia\\n2. AWS, Virginia\\n3. AWS, California\\n\\nAnd putting all the winners in a table:\\n\\n| Region | Hosting Provider |\\n| ------ | ---------------- |\\n| us-east-1    | AWS, Virginia     |\\n| eu-central-1 | Hetzner, Frankfurt |\\n| us-east-2 | AWS, Ohio |\\n| eu-west-3 | Google Cloud, Brussels |\\n| eu-west-2 | OVH, UK |\\n| ap-northeast-1 | AWS, Tokyo |\\n| ap-southeast-1 | Contabo, Singapore |\\n| eu-west-1 | AWS, Dublin |\\n| us-west-2 | Telus Communications, Vancouver |\\n| ap-east-1 | Google Cloud, Hong Kong |\\n| ap-northeast-2 | AWS, Seoul |\\n| eu-north-1 | Hetzner, Frankfurt |\\n| us-west-1 | AWS, Virginia |\\n\\nTurns out that **AWS wins only in 6 of the 13 regions**. In Virginia and in Ohio, both very important regions, it has been the biggest broadcaster. But we also see Google Cloud winning in Hong Kong as well as in Paris (from their datacenter in Brussels, Belgium). Hetzner wins in Frankfurt by a wide margin, perhaps not surprisingly. It also wins in Stockholm, Sweden,\\neven though the broadcaster was in Frankfurt. This tells us that the eu-north area does not seem to have a lot of\\nactivity. Overall, Hetzner is a really active hosting provider in Europe. \\nIn Oregon, the biggest broadcaster was in Vancouver, Canada. In Singapore, it came from Contabo.\\n\\nAnother interesting observation is that there\'s almost never one broadcaster that stands out per region. The second and\\nthird biggest broadcasters are never far behind. This suggests that providers like [Alchemy](https://www.alchemy.com/)\\nand [Infura](https://infura.io/), who send the largest part of Ethereum transactions, use a lot of nodes, possibly\\ndistributed across geographic areas.\\n\\nYou might have also noticed the slight anomalies tagged with **(?)**. These are entries that don\'t really make a lot of sense.\\nThe fact that an AWS broadcaster in Virginia can make the top 3 active peers of our node in Hong Kong \\nmeans that those transactions didn\'t arrive at our node in Virginia first. For now, we attribute these anomalies to the randomness\\nof the p2p network, but we\'ll continue looking into it.\\n\\n## Conclusion\\nWhen milliseconds matter, this is all information that has to be taken into consideration, and that\'s what we did\\nwith Fiber. Listening to transactions is only one aspect of high frequency trading on blockchains, but it\'s a very important one that one needs to get right by carefully choosing the regions, concentration, and cloud providers\\nacross which to deploy nodes.\\n\\nIf you\'re interested in learning more about Fiber, check out the [documentation](/docs/intro) or\\njoin the [Discord](https://discord.gg/J4KNdeCYGX). We will be posting more stories like these,\\nfollow us on Twitter for updates [@chainbound_](https://twitter.com/chainbound_)!\\n\\n## References\\n1. https://ethernodes.org/networkType/Hosting\\n2. In reality, the full message is only broadcast to a subset of peers, whereas the rest receive only the hash **announcement**.\\nIt\'s up to these nodes to check if they\'ve already received the transaction, and otherwise they have to request\\nit. This introduces another round trip. See the devp2p specificiation on transaction exchange: https://github.com/ethereum/devp2p/blob/master/caps/eth.md#transaction-exchange"}]}')}}]);